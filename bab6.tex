%-----------------------------------------------------------------------------%
\chapter{\babEnam}
%-----------------------------------------------------------------------------%

Bagian ini berisi kesimpulan dari penelitian dan saran untuk penelitian selanjutnya.

%---------------------------------------------------------------
\section{Kesimpulan}
%---------------------------------------------------------------
OpenCL dapat digunakan untuk menjalankan proses \textit{inference} pada \textit{Deep Learning} pada \textit{mobile} GPU dengan cara mengimplementasikan program untuk operasi-operasi matriks pada proses \textit{inference} menggunakan OpenCL. Untuk melakukannya, diperlukan pustaka OpenCL dan OpenCL \textit{headers} yang dapat diperoleh dari masing-masing vendor GPU. Hasil implementasi operasi-operasi matriks pada proses \textit{inference} menggunakan OpenCL dapat diintegrasikan ke Tensorflow Lite yang merupakan pustaka \textit{Deep Learning} yang bersifat sumber terbuka. Hasil implementasi tersebut dapat ditambahkan sebagai \textit{kernel} baru pada Tensorflow Lite. Pada penelitian ini penulis telah berhasil menambahkan OpenCL \textit{kernel} pada Tensorflow Lite untuk operasi perkalian matriks-matriks dan operasi konvolusi matriks yang berjalan di GPU.

Pengujian telah dilakukan terhadap OpenCL \textit{kernel} yang telah diimplementasikan. Hasil pengujian menunjukkan bahwa OpenCL \textit{kernel} yang berjalan di GPU untuk operasi perkalian matriks-matriks memiliki kecepatan yang lebih baik daripada \textit{naive kernel} dan \textit{optimized kernel} dari Tensorflow Lite yang berjalan di CPU. Keunggulan kecepatan dari OpenCL \textit{kernel} semakin terlihat ketika ukuran matriks masukan semakin besar. Sementara itu pada operasi konvolusi matriks, OpenCL \textit{kernel} memiliki kecepatan yang lebih baik daripada dua \textit{kernel} lain ketika banyaknya \textit{batch} dan kedalaman dari matriks keluaran relatif kecil. Saat banyaknya \textit{batch} dan kedalaman matriks keluaran cukup besar, kecepatan dari OpenCL \textit{kernel} tidak lebih baik daripada \textit{optimized kernel} yang berjalan di CPU.

Pada dua operasi matriks yang telah diuji, diketahui bahwa proses transfer data antar memori pada OpenCL cukup berpengaruh terhadap kecepatan OpenCL \textit{kernel}. Pada matriks-matriks yang relatif kecil, proses transfer data ini menjadi \textit{bottleneck}. \textit{Bottleneck} pada transfer data antar memori tersebut semakin menghilang ketika ukuran matriks masukan semakin besar. Pada matriks-matriks yang cukup besar, waktu yang diperlukan untuk eksekusi \textit{kernel} jauh lebih besar daripada waktu yang diperlukan untuk transfer data antara memori CPU dan GPU. Salah satu penyebabnya adalah semakin banyaknya operasi baca/tulis terhadap memori GPU yang dilakukan oleh \textit{kernel} ketika ukuran matriks semakin besar. 

%---------------------------------------------------------------
\section{Saran}
%-------------------
Pada penelitian ini penulis telah mengimplementasikan program OpenCL untuk operasi perkalian matriks-matriks dan konvolusi matriks. Batasan pada penelitian ini adalah operasi konvolusi matriks yang telah diimplementasikan menggunakan OpenCL hanya dapat digunakan ketika besarnya jangkah pada konvolusi adalah satu. Ukuran jangkah yang lebih besar dari satu menyebabkan blok matriks masukan yang diproses oleh suatu \textit{work-group} menjadi lebih besar. Jika blok terlalu besar, \textit{local memory caching} tidak dapat dilakukan karena keterbatasan kapasitas memori lokal. Bahkan ketika besarnya jangkah sama dengan satu, tidak semua GPU dapat menjalankan OpenCL \textit{kernel} akibat keterbatasan memori. Untuk penelitian selanjutnya, disarankan untuk mengimplementasikan program OpenCL untuk operasi konvolusi yang mendukung semua ukuran jangkah, namun tetap memiliki kecepatan yang baik. Untuk melakukannya mungkin diperlukan pendekatan lain selain \textit{local memory caching} agar kapasitas memori tidak menjadi masalah.

Program OpenCL pada penelitian ini hanya diimplementasikan untuk perangkat Android. Selain untuk Android, Tensorflow Lite dan OpenCL sebenarnya juga dapat dijalankan pada perangkat IoS. Penulis menyarankan untuk mencoba menggunakan OpenCL untuk menjalankan proses \textit{inference} pada perankat IoS. Untuk perangkat Android sendiri, eksekusi proses \textit{inference}  di GPU sebenarnya tidak harus melalui OpenCL. Terdapat pilihan API lain seperti Vulkan \cite{vulkan} dan OpenGL \cite{opengl} untuk melakukan pemrograman paralel di GPU. Penulis sebenarnya telah mengimplementasikan Tensorflow Lite \textit{kernel} menggunakan Vulkan untuk operasi perkalian matriks-matriks dan konvolusi matriks dengan menggunakan pendekatan yang sama seperti yang telah dilakukan menggunakan OpenCL. Meskipun pendekatannya sama, kecepatan yang dihasilkan kurang memuaskan. Pada beberapa kasus, Vulkan \textit{kernel} bahkan tidak dapat mengungguli \textit{naive kernel} dari Tensorflow Lite. Penulis menyarankan untuk melakukan penelitian lebih lanjut mengenai penggunaan Vulkan untuk menjalankan proses \textit{inference} pada \textit{Deep Learning}.