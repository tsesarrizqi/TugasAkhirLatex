%
% Halaman Abstrak
%
% @author  Andreas Febrian
% @version 1.00
%

\chapter*{Abstrak}

\vspace*{0.2cm}

\noindent \begin{tabular}{l l p{10cm}}
	Nama&: & \penulis \\
	Program Studi&: & \program \\
	Judul&: & \judul \\
\end{tabular} \\ 

\vspace*{0.5cm}

\noindent 
Proses \textit{inference} pada \textit{Deep Learning} saat ini sudah dapat dijalankan pada perangkat \textit{mobile}. Tensorflow Lite adalah \textit{library} yang dapat digunakan untuk melakukan proses \textit{inference} pada perangkat \textit{mobile}. Proses \textit{inference} melalui Tensorflow Lite saat ini hanya dapat dilakukan di CPU. Penulis melihat peluang penggunaan \textit{mobile} GPU untuk menjalankan operasi-operasi matriks pada proses \textit{inference} melalui Tensorflow Lite. Penulis telah mengimplementasikan operasi perkalian matriks-matriks dan konvolusi matriks menggunakan OpenCL dan mengintegrasikannya ke Tensorflow Lite sehingga operasi-operasi tersebut dapat berjalan di GPU ketika proses \textit{inference} pada Tensorflow Lite berlangsung. Hasil pengujian yang dilakukan menunjukkan bahwa penggunaan OpenCL untuk menjalankan operasi perkalian matriks-matriks di GPU dapat meningkatkan kecepatan proses \textit{inference} pada Tensorflow Lite, terutama ketika matriks berukuran besar. Sementara itu, penggunaan OpenCL untuk menjalankan untuk menjalankan operasi konvolusi matriks juga mampu meningkatkan kecepatan proses \textit{inference} dalam beberapa kasus matriks masukan.
\\

\vspace*{0.2cm}

\noindent Kata Kunci: \\ 
\noindent Deep Learning; Mobile GPU; OpenCL; \\

\newpage