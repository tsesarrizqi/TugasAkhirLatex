%
% Halaman Abstract
%
% @author  Andreas Febrian
% @version 1.00
%

\chapter*{ABSTRACT}

\vspace*{0.2cm}

\noindent \begin{tabular}{l l p{11.0cm}}
	Name&: & \penulis \\
	Program&: & \programInggris \\
	Title&: & \judulInggris \\
\end{tabular} \\ 

\vspace*{0.5cm}

\noindent 
Today, mobile device can be used to run the inference process of Deep Learning. Tensorflow Lite is a Machine Learning library that can be used for this task. Currently, Tensorflow Lite only supports the use of mobile CPU to run Deep Learning inference. There is possibility that \textit{mobile} GPU can be used to execute matrix operastions that are part of Deep Learning inference. We have implemented OpenCL programs for matrix-matrix multiplication and matrix convolution and integrate them with Tensorflow Lite so that they can be executed in mobile GPU via inference process in Tensorflow Lite. Several experiments that have been done showed that using OpenCL to execute matrix-matrix multiplication in mobile GPU can increase inference speed when the matrices are large enough. Using GPU for matrix convolution via OpenCL in some cases can also increase the speed of inference.
\\

\vspace*{0.2cm}

\noindent Keywords: \\ 
\noindent Deep Learning; Mobile GPU; OpenCL; \\

\newpage