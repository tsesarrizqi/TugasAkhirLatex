%-----------------------------------------------------------------------------%
\chapter{\babSatu}
%-----------------------------------------------------------------------------%
Karya tulis yang berjudul "\judul" ini didahului dengan pembahasan mengenai latar belakang penelitian, permasalahan yang ingin diselesaikan, dan tujuan dari penelitian.

%-----------------------------------------------------------------------------%
\section{Latar Belakang}
%-----------------------------------------------------------------------------%
\textit{Deep Learning} merupakan algoritma \textit{Machine Learning} yang diketahui memiliki akurasi tinggi. \textit{Deep Learning} memanfaatkan arsitektur \textit{Neural Network} yang telah dilatih menggunakan sekumpulan data untuk melakukan prediksi skor atau label dari suatu data baru. Popularitas \textit{Deep Learning} meningkat pesat dalam beberapa tahun terakhir. Salah satu jenis arsitektur \textit{Deep Learning}, \textit{Convolutional Neural Network} (CNN), memiliki peran besar dalam meningkatnya popularitas \textit{Deep Learning}. CNN biasa digunakan dalam aplikasi pengenalan citra. Selain CNN, dalam \textit{Deep Learning} juga terdapat jenis model lain seperti \textit{Long Short-Term Memory} (LSTM) yang biasa digunakan untuk menyelesaikan permasalahan di bidang pengolahan bahasa.

Seiring meningkatnya popularitas \textit{Deep Learning}, dukungan terus bermunculan dari berbagai pihak terutama yang terkait dengan peningkatan performa \textit{Deep Learning}. Saat ini CPU tidak lagi menjadi pilihan utama untuk menjalankan \textit{Deep Learning} pada komputer personal atau \textit{server}. GPU telah diketahui dapat menjalankan \textit{Deep Learning} dengan performa komputasi yang jauh lebih baik daripada CPU, terutama ketika melakukan \textit{training}. Selain dalam hal performa, dukungan untuk mobilitas Deep Learning juga mulai bermunculan. Belum lama ini, Google merilis \textit{open-source library} bernama Tensorflow Lite \cite{tflite} yang memungkinkan pengguna menjalankan \textit{Deep Learning inference} pada perangkat \textit{mobile} dengan performa tinggi.

Saat ini operasi-operasi \textit{Deep Learning inference} pada Tensorflow Lite hanya dapat dijalankan di CPU. Meskipun demikian performa yang dihasilkan sudah sangat baik. Pada Tugas Akhir ini penulis melakukan penelitian mengenai penggunaan GPU untuk menjalankan \textit{Deep Learning inference} pada perangkat \textit{mobile} melalui Tensorflow Lite. Penulis mengimplementasikan beberapa Tensorflow Lite \textit{kernel} untuk beberapa operasi matriks pada \textit{inference} yang berjalan di GPU. Menjalankan \textit{Deep Learning inference} pada \textit{mobile} GPU bukanlah hal yang baru. Sudah ada \textit{library} bernama CNNdroid \cite{cnndroid} yang dapat digunakan untuk menjalankan CNN \textit{inference} pada \textit{mobile} GPU. CNNdroid memanfaatkan Renderscript untuk menjalankan komputasi pada GPU.

Dalam penelitian ini penulis menggunakan OpenCL untuk menjalankan \textit{Deep Learning inference} pada \textit{mobile} GPU. OpenCL \cite{opencl} merupakan API pemrograman paralel untuk berbagai jenis prosesor seperti CPU, GPU, dan FPGA. OpenCL merupakan API pemrograman paralel yang mendukung komputasi dengan paradigma SIMT (\textit{Single Instruction Multiple Thread}). \textit{Deep Learning inference} terdiri dari operasi-operasi matriks yang berpotensi untuk diimplementasikan secara SIMT pada GPU yang memiliki sangat banyak \textit{compute unit} (\textit{thread}). Implementasi \textit{Deep Learning inference} pada \textit{mobile} GPU melalui OpenCL diharapkan memberikan performa yang baik.

%-----------------------------------------------------------------------------%
\section{Permasalahan}
%-----------------------------------------------------------------------------%
Pada bagian ini akan dijelaskan mengenai definisi permasalahan 
yang penulis hadapi dan ingin diselesaikan serta asumsi dan batasan yang digunakan dalam menyelesaikannya.
%-----------------------------------------------------------------------------%
\subsection{Definisi Permasalahan}
%-----------------------------------------------------------------------------%
Berikut adalah permasalahan-permasalahan yang akan dijawab dalam penelitian ini.
\begin{enumerate}
\item Apakah OpenCL dapat digunakan untuk menjalankan operasi-operasi \textit{inference} di GPU melalui Tensorflow Lite?
\item Bagaimana perbandingan kecepatan Tensorflow Lite \textit{kernel} yang berjalan di GPU melalui OpenCL dengan Tensorflow Lite \textit{kernel} yang berjalan di CPU?
\item Dimanakah letak \textit{bottleneck} (pada komputasi atau pada baca/tulis memori) dari penggunaan OpenCL untuk menjalankan operasi-operasi \textit{inference} di GPU?

\end{enumerate}

%-----------------------------------------------------------------------------%
\subsection{Batasan Permasalahan}
%-----------------------------------------------------------------------------%
Berikut adalah batasan dan asumsi pada penelitian.
\begin{enumerate}
\item Implementasi Tensorflow Lite \textit{kernel} yang menggunakan OpenCL hanya dapat digunakan untuk perangkat \textit{Android} saja.
\item Penulis hanya mengimplementasikan Tensorflow Lite \textit{kernel} melalui OpenCL untuk dua operasi \textit{inference} saja yaitu perkalian konvolusi matriks dan perkalian matriks-matriks.
\item Implementasi konvolusi matriks hanya dapat digunakan untuk konvolusi dengan \textit{stride} sebesar 1 ke kanan dan 1 ke bawah.
\item Hasil implementasi OpenCL hanya dapat digunakan untuk \textit{convolution layer} dan \textit{fully-connected layer} saja.
\item Penulis mengasumsikan bahwa perangkat yang digunakan hanya menggunakan sumber daya \textit{multi-core} CPU dan GPU beserta memorinya, terlepas dari dorongan performa yang berasal dari perangkat tambahan.
\end{enumerate}

%-----------------------------------------------------------------------------%
\section{Tujuan}
%-----------------------------------------------------------------------------%
Tujuan dari penelitian ini adalah sebagai berikut.
\begin{enumerate}
\item Mengimplementasikan operasi-operasi \textit{Deep Learning inference} berupa Tensorflow Lite \textit{kernel} menggunakan OpenCL agar dapat dijalankan di GPU.
\item Membandingkan kecepatan Tensorflow Lite \textit{kernel} \textit{inference} yang berjalan di GPU melalui OpenCL dengan Tensorflow Lite \textit{kernel} yang berjalan di CPU.
\item Mengetahui letak \textit{bottleneck} (pada komputasi atau pada baca/tulis memori) dari penggunaan OpenCL untuk menjalankan operasi-operasi \textit{inference} di GPU.

\end{enumerate}

%-----------------------------------------------------------------------------%
%\section{Posisi Penelitian}
%-----------------------------------------------------------------------------%


