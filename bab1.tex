%-----------------------------------------------------------------------------%
\chapter{\babSatu}
%-----------------------------------------------------------------------------%
Karya tulis yang berjudul "\judul" ini didahului dengan pembahasan mengenai latar belakang penelitian, permasalahan yang ingin diselesaikan, dan tujuan dari penelitian.

%-----------------------------------------------------------------------------%
\section{Latar Belakang}
%-----------------------------------------------------------------------------%
\textit{Deep Learning} merupakan teknik \textit{Machine Learning} yang mampu mempelajari representasi data secara otomatis melalui suatu model \textit{Neural Network} yang terdiri dari beberapa lapisan pemrosesan \cite{deeplearning}. Model \textit{Deep Learning} dapat menerima masukan berupa data mentah untuk melakukan klasifikasi. Model \textit{Deep Learning} ditingkatkan akurasinya melalui proses latihan dan digunakan untuk memprediksi label atau kelas dari suatu data melalui proses \textit{inference}. Salah satu jenis model \textit{Deep Learning}, \textit{Convolutional Neural Network} (CNN) \cite{cnnori}, memiliki peran besar dalam meningkatnya popularitas \textit{Deep Learning} dalam beberapa tahun terakhir. CNN memiliki kemampuan yang sangat baik dalam memproses data berupa citra, video, maupun audio. Selain CNN, dalam \textit{Deep Learning} juga terdapat jenis model lain seperti \textit{Long Short-Term Memory} (LSTM) \cite{lstm} yang dapat digunakan untuk melakukan pemrosesan data bersifat sekuensial.

Seiring meningkatnya popularitas \textit{Deep Learning}, usaha-usaha untuk meningkatkan kecepatan proses latihan maupun \textit{inference} pada \textit{Deep Learning} terus bermunculan. Saat ini CPU tidak lagi menjadi pilihan utama untuk menjalankan proses latihan maupun \textit{inference} pada komputer personal atau \textit{server}. Perangkat lunak Deep Learning untuk komputer personal atau \textit{server} saat ini sudah dapat menjalankan proses-proses \textit{Deep Learning} di GPU. Proses latihan dan \textit{inference} pada \textit{Deep Learning} mengandung operasi-operasi matriks yang berat seperti perkalian matriks-matriks dan konvolusi matriks. GPU yang memiliki performa komputasi yang tinggi dapat menjalankan operasi-operasi tersebut dengan lebih cepat. Usaha juga dilakukan untuk membawa \textit{Deep Learning} kepada perangkat yang lebih kecil, yaitu perangkat \textit{mobile}. Belum lama ini, Google merilis perangkat lunak sumber terbuka bernama Tensorflow Lite \cite{tflite} yang memungkinkan pengguna menjalankan proses \textit{inference} pada \textit{Deep Learning} pada perangkat \textit{mobile}.

Tensorflow Lite saat ini hanya dapat menjalankan proses \textit{inference} di CPU. Penulis melihat peluang penggunaan GPU untuk menjalankan proses \textit{inference} pada Tensorflow Lite, yaitu dengan cara mengimplementasikan program untuk operasi-operasi matriks pada proses \textit{inference} yang berjalan di GPU dan mengintegrasikannya ke Tensorflow Lite. Penulis ingin mengetahui apakah penggunaan \textit{mobile} GPU untuk menjalankan operasi-operasi matriks tersebut dapat meningkatkan kecepatan proses \textit{inference} pada Tensorflow Lite. Menjalankan proses \textit{inference} di \textit{mobile} GPU sebenarnya bukanlah hal yang baru. Sudah ada perangkat lunak bernama CNNdroid \cite{cnndroid} yang dapat digunakan untuk menjalankan proses \textit{inference} di GPU untuk model CNN. CNNdroid memanfaatkan Renderscript untuk menjalankan komputasi pada GPU.

Dalam penelitian ini penulis mencoba menggunakan OpenCL \cite{openclori} untuk mengimplementasikan program untuk operasi-operasi matriks pada proses \textit{inference} yang berjalan di GPU. OpenCL merupakan API pemrograman paralel untuk berbagai jenis prosesor seperti CPU, GPU, dan FPGA \cite{opencl}. Program yang diimplementasikan menggunakan OpenCL memiliki portabilitas tinggi karena dapat dijalankan pada berbagai prosesor yang berbeda-beda. OpenCL merupakan API pemrograman paralel yang mendukung komputasi dengan menggunakan banyak \textit{thread}. GPU merupakan prosesor dengan ratusan \textit{core} yang mendukung komputasi secara paralel. Penggunaan \textit{mobile} GPU untuk menjalankan operasi-operasi matriks pada proses \textit{inference} diharapkan dapat meningkatkan kecepatan proses \textit{inference} pada Tensorflow Lite.

%-----------------------------------------------------------------------------%
\section{Permasalahan}
%-----------------------------------------------------------------------------%
Pada bagian ini akan dijelaskan mengenai definisi permasalahan 
yang penulis hadapi dan ingin diselesaikan serta asumsi dan batasan yang digunakan dalam menyelesaikannya.
%-----------------------------------------------------------------------------%
\subsection{Definisi Permasalahan}
%-----------------------------------------------------------------------------%
Berikut adalah permasalahan-permasalahan yang akan dijawab dalam penelitian ini.
\begin{enumerate}
\item Apakah OpenCL dapat digunakan untuk menjalankan operasi-operasi matriks pada proses \textit{inference} di GPU pada Tensorflow Lite?
\item Bagaimana perbandingan kecepatan operasi-operasi matriks pada proses \textit{inference} antara yang berjalan di GPU (melalui OpenCL) dengan yang berjalan di CPU (implementasi asli Tensorflow Lite)?
\end{enumerate}

%-----------------------------------------------------------------------------%
\subsection{Batasan Permasalahan}
%-----------------------------------------------------------------------------%
Berikut adalah batasan dan asumsi pada penelitian.
\begin{enumerate}
\item Penulis hanya mengimplementasikan dua operasi matriks pada proses \textit{inference} menggunakan OpenCL, yaitu operasi perkalian matriks-matriks dan operasi konvolusi matriks.
\item Program OpenCL yang telah diimplementasikan hanya dapat digunakan untuk perangkat \textit{Android} saja.
\item Operasi konvolusi matriks yang diimplementasikan menggunakan OpenCL diasumsikan memiliki jangkah sebesar satu.
\item Penulis mengasumsikan bahwa perangkat yang digunakan hanya menggunakan sumber daya \textit{multi-core} CPU dan GPU beserta memorinya, terlepas dari dorongan performa yang berasal dari perangkat tambahan.
\end{enumerate}

%-----------------------------------------------------------------------------%
\section{Tujuan}
%-----------------------------------------------------------------------------%
Tujuan dari penelitian ini adalah sebagai berikut.
\begin{enumerate}
\item Mengimplementasikan operasi-operasi matriks pada proses \textit{inference} pada Tensorflow Lite menggunakan OpenCL sehingga dapat berjalan di GPU.
\item Membandingkan kecepatan operasi-operasi matriks pada proses \textit{inference} antara yang berjalan di GPU (melalui OpenCL) dengan yang berjalan di CPU (implementasi asli Tensorflow Lite)?
\end{enumerate}

%-----------------------------------------------------------------------------%
%\section{Posisi Penelitian}
%-----------------------------------------------------------------------------%


